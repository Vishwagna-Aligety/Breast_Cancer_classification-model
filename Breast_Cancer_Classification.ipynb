{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2pPGYG7P5Ff8JslKenFXT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishwagna-Aligety/Breast_Cancer_classification-model/blob/main/Breast_Cancer_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovs_o2RPC_TQ"
      },
      "outputs": [],
      "source": [
        "                  #  Breast Cancer Classification Using CNN (CancerNet)\n",
        "            #**Objective**: To build a Convolutional Neural Network (CNN) model named *CancerNet* to classify breast cancer histology images (benign or malignant) using the IDC dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ STEP 1: Setup Environment ------------------\n",
        "from google.colab import files\n",
        "files.upload()  # Upload your kaggle.json API key\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download IDC dataset from Kaggle\n",
        "!kaggle datasets download -d paultimothymooney/breast-histopathology-images\n",
        "\n",
        "# Extract dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"breast-histopathology-images.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/IDC_dataset\")"
      ],
      "metadata": {
        "id": "2LzZOzhuDJjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ STEP 2: Organize Dataset ------------------\n",
        "import os, shutil, random\n",
        "\n",
        "base_dir = \"/content/IDC_dataset\"\n",
        "dataset_root = os.path.join(base_dir, \"IDC_regular_ps50_idx5\")\n",
        "\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "val_dir   = os.path.join(base_dir, \"val\")\n",
        "test_dir  = os.path.join(base_dir, \"test\")\n",
        "\n",
        "for d in [train_dir, val_dir, test_dir]:\n",
        "    os.makedirs(os.path.join(d,\"0\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(d,\"1\"), exist_ok=True)\n",
        "\n",
        "# Split dataset manually (70% train, 20% val, 10% test)\n",
        "all_images = []\n",
        "for root, dirs, files in os.walk(dataset_root):\n",
        "    for file in files:\n",
        "        if file.endswith(\".png\"):\n",
        "            all_images.append(os.path.join(root, file))\n",
        "\n",
        "random.shuffle(all_images)\n",
        "train_split = int(0.7*len(all_images))\n",
        "val_split   = int(0.9*len(all_images))\n",
        "\n",
        "train_files = all_images[:train_split]\n",
        "val_files   = all_images[train_split:val_split]\n",
        "test_files  = all_images[val_split:]\n",
        "\n",
        "def copy_files(file_list, target_dir):\n",
        "    for f in file_list:\n",
        "        label = \"1\" if \"class1\" in f or \"_1\" in f else \"0\"\n",
        "        shutil.copy(f, os.path.join(target_dir,label,os.path.basename(f)))\n",
        "\n",
        "copy_files(train_files, train_dir)\n",
        "copy_files(val_files, val_dir)\n",
        "copy_files(test_files, test_dir)"
      ],
      "metadata": {
        "id": "ZxPPb8ugDRT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ STEP 3: Data Generators with Augmentation ------------------\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen_train = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=15,\n",
        "                                   width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1,\n",
        "                                   horizontal_flip=True)\n",
        "datagen_val   = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "img_size = (50,50)\n",
        "batch = 64\n",
        "\n",
        "train_gen = datagen_train.flow_from_directory(train_dir, target_size=img_size, batch_size=batch, class_mode=\"binary\")\n",
        "val_gen   = datagen_val.flow_from_directory(val_dir, target_size=img_size, batch_size=batch, class_mode=\"binary\")\n",
        "test_gen  = datagen_val.flow_from_directory(test_dir, target_size=img_size, batch_size=batch, class_mode=\"binary\", shuffle=False)"
      ],
      "metadata": {
        "id": "hFuWTj98DlGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ STEP 4: Define CNN (Improved to Reduce Overfitting) ------------------\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32,(3,3),activation='relu',input_shape=(50,50,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64,(3,3),activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128,activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1,activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "PAMamjJnD6MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ STEP 5: Train Model with Early Stopping ------------------\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(train_gen, epochs=10, validation_data=val_gen, callbacks=[early_stop])"
      ],
      "metadata": {
        "id": "c-ulMGl8ECe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ STEP 6: Evaluate Model ------------------\n",
        "loss, acc = model.evaluate(test_gen)\n",
        "print(f\"Final Test Accuracy: {acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "pcC_kQrqEJol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D-liqxw_sOIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ STEP 7: Confusion Matrix ------------------\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = (model.predict(test_gen) > 0.5).astype(\"int32\")\n",
        "cm = confusion_matrix(test_gen.classes, y_pred)\n",
        "ConfusionMatrixDisplay(cm, display_labels=[\"Benign\",\"Malignant\"]).plot()"
      ],
      "metadata": {
        "id": "_VzJGV_OEW_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ STEP 8: Save Model ------------------\n",
        "model.save(\"CancerNet_Improved.h5\")"
      ],
      "metadata": {
        "id": "wx2msPCnJzUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}